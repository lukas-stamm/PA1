{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from datetime import datetime   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA LOADING AND INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LineId             0\n",
       "Month              0\n",
       "Date               0\n",
       "Time               0\n",
       "Level              0\n",
       "Component          0\n",
       "PID              151\n",
       "Content            0\n",
       "EventId            0\n",
       "EventTemplate      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the structured log data\n",
    "structured_log_df = pd.read_csv(\"Linux_2k.log_structured.csv\")\n",
    "structured_log_df\n",
    "# Load template csv for labeling insights\n",
    "template_df = pd.read_csv(\"Linux_2k.log_templates.csv\")\n",
    "template_df\n",
    "\n",
    "# Check if required missing\n",
    "missing_values = structured_log_df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling\n",
    "# Generalized criteria for harmful log messages\n",
    "harmful_criteria = [\n",
    "    \"authentication failure\",\n",
    "    \"user unknown\",\n",
    "    \"permission denied\",\n",
    "    \"error\",\n",
    "    \"failed\",\n",
    "    \"denied\",\n",
    "    \"abnormal\",\n",
    "    \"critical\"\n",
    "    ] \n",
    "structured_log_df[\"Label\"] = structured_log_df[\"Content\"].apply(lambda x: \"harmful\" if any(criterion in str(x) for criterion in harmful_criteria) else \"non-harmful\"\n",
    ")                                                                             \n",
    "structured_log_df.to_csv(\"labeled.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping 'Label' to numerical values: \"harmful\" -> 1, \"normal\" -> 0\n",
    "label_mapping = {\"harmful\": -1, \"non-harmful\": 1}\n",
    "structured_log_df[\"Label\"] = structured_log_df[\"Label\"].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming a default year for the datetime conversion\n",
    "default_year = 2023\n",
    "\n",
    "# Creating a new column 'Timestamp' by concatenating 'Month', 'Date', 'Time', and the default year\n",
    "structured_log_df[\"Timestamp\"] = structured_log_df.apply(lambda row: datetime.strptime(f\"{default_year} {row['Month']} {row['Date']} {row['Time']}\", '%Y %b %d %H:%M:%S'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Content\n",
    "def clean_text(text):\n",
    "    # Remove special characters and symbols\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    \n",
    "    # Remove extra white spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'Content' column\n",
    "structured_log_df[\"Content\"] = structured_log_df[\"Content\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply the tokenization function to the 'Content' column\n",
    "structured_log_df[\"Content\"] = structured_log_df[\"Content\"].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop word removal\n",
    "# Defining a basic list of English stop words\n",
    "basic_stopwords = set([\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
    "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
    "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
    "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\",\n",
    "    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
    "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
    "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
    "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
    "    \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
    "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "])\n",
    "\n",
    "# Removing stop words\n",
    "structured_log_df[\"Content\"] = structured_log_df[\"Content\"].apply(lambda x: [word for word in x if word not in basic_stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lukas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmitization\n",
    "# Download the necessary NLTK data \n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to get the WordNet part of speech tag\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if unknown\n",
    "\n",
    "# Function to lemmatize a list of tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    # Get the part of speech for each token\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Apply lemmatization considering the part of speech\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in pos_tags]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Apply the lemmatization function\n",
    "structured_log_df[\"Content\"] = structured_log_df[\"Content\"].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\python3106\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 340)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Since the data is already tokenized, we define a tokenizer function that just returns its input\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer with the identity tokenizer\n",
    "vectorizer = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False)\n",
    "\n",
    "# Fit and transform the 'Content' column to get the TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(structured_log_df[\"Content\"])\n",
    "\n",
    "# Show the shape of the TF-IDF matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Fit and transform the 'Content' column to get the TF-IDF features\n",
    "X = tfidf_matrix\n",
    "\n",
    "# The labels are directly taken from the 'Label' column\n",
    "y = structured_log_df[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=100, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=100, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=auto, kernel=rbf; total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"kernel\": [\"linear\", \"rbf\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "# Initialize the Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=2, cv=5)\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Find the best parameters\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "# Retrieve the best model from the grid search\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "best_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[133   0]\n",
      " [  0 267]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       133\n",
      "           1       1.00      1.00      1.00       267\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on training split\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
